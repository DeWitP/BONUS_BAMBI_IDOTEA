Full-blown GATK pipeline for 2bRAD data, based on bowtie2 mapping to a reference genome
September 14, 2014, Mikhail Matz matz@utexas.edu

# setting up the genome reference

# concatenating genome contigs into small number of "pseudo-chromosomes" (to help memory usage in GATK).
# Using the scaffolded Idotea assembly which has been run through the redundans pipeline:

~/scripts/concatFasta.pl fasta=Idotea_0002_reduced_pruned_arcs_scaffold.fa

# normalize fasta records to ensure all lines are of the same length, using Picard

module load picard/v2.1.1

java -Xmx1g -jar ~/scripts/NormalizeFasta.jar INPUT=Idotea_0002_reduced_pruned_arcs_scaffold_cc.fasta OUTPUT=Idotea_0002_reduced_pruned_arcs_scaffold_ccn.fasta

# One possibility is to edit the env variables to make the software easier find the genome and software.
# If so, edit these lines THROUGHOUT THE TEXT to fit the location and name of your genome reference 
#export GENOME_FASTA=Idotea_0002_reduced_pruned_arcs_scaffold_ccn.fasta
#export GENOME_DICT=Idotea_0002_reduced_ccn.dict
#export GENOME_PATH=/nobackup/data7/pierre/IB14/
#export GENOME_REF=/nobackup/data7/pierre/IB14/Idotea_0002_reduced_pruned_arcs_scaffold_ccn.fasta
#export TACC_GATK_DIR=~/scripts/
#export TACC_PICARD_DIR=~/scripts/


module load Bowtie2/v2.2.7

# creating genome indexes:
bowtie2-build Idotea_0002_reduced_pruned_arcs_scaffold_ccn.fasta Idotea_0002_reduced_pruned_arcs_scaffold_ccn.fasta

module load samtools
samtools faidx Idotea_0002_reduced_pruned_arcs_scaffold_ccn.fasta

java -jar ~/scripts/CreateSequenceDictionary.jar R=Idotea_0002_reduced_pruned_arcs_scaffold_ccn.fasta  O=Idotea_0002_reduced_pruned_arcs_scaffold_ccn.dict

#----------------------------------
# concatenating the reads files (make sure to unzip them first!):

~/scripts/ngs_concat.pl fastq "IB14(.+)_S\d+"

# Trimming the reads:

~/scripts/2bRAD_trim_launch.pl fq > trims.sh

#Then open the file and make sure the path to 2bRADtrim.pl is correct, and add a header line #!/bin/bash

chmod 755 trims.sh
./trims.sh

# quality filtering using fastx_toolkit
module load fastx_toolkit
ls *.tr0 | perl -pe 's/^(\S+)\.tr0$/cat $1\.tr0 \| fastq_quality_filter -q 20 -p 90 >$1\.trim/' >filt0
cat filt0 | perl -pe 's/filter /filter -Q33 /' > filt.sh

#Then open the file and add a header line #!/bin/bash

chmod 755 filt.sh
./filt.sh

#----------------------------------
# mapping reads and reformatting mapped data files

# aligning with bowtie2 :

~/scripts/2bRAD_bowtie2_launch.pl '\.trim$' /nobackup/data7/pierre/IB14/Arcs_GATK/Idotea_0002_reduced_pruned_arcs_scaffold_ccn.fasta > bt2_aln.sge

# You can add a multithreading flag (-p 16) to the sge script with:

sed -i 's/-L 16/-L 16 -p 16/g' bt2.sge

#Then open the file and make sure the path is correct, and add a header:
#$ -cwd
#$ -q high_mem
#$ -S /bin/bash
#$ -pe mpich 16
module load Bowtie2/v2.2.7

# Re-save the file, then submit the job to the high_mem queue:

qsub bt2.sge

ls *.bt2.sam > sams
cat sams | wc -l  
# do you have sams for all your samples?... If not, rerun the chunk above

# making bam files

~/scripts/AddReadGroups.sh
~/scripts/convert_to_bam.sh

#rm *sorted*
#ls *bt2.bam > bams
#cat bams | wc -l  
# do you have bams for all your samples?... If not, rerun the chunk above

#----------------------------------
# WARNING!!! DO NOT RUN THIS FOR RAD!!! SKIPPING THIS SECTION!
# marking duplicate reads 
# module load picard
# export GENOME_REF=where/genome/is/mygenome_ccn.fasta
#cat bams | perl -pe 's/(\S+)\.bam/java -Xmx4g -jar ~\/scripts\/MarkDuplicates\.jar INPUT=$1\.bam OUTPUT=$1\.dedup.bam METRICS_FILE=$1\.metrics AS=true CREATE_INDEX=true/' > dd
#launcher_creator.py -j dd -n dd -l dd.job -q normal
#cat dd.job | perl -pe 's/12way \d+/4way NNN/' > ddup.job  # REPLACE NNN WITH 12*ceiling([number of bam files]/4)
#qsub ddup.job
#ls *.dedup.bam > bams

#----------------------------------
# starting GATK
# realigning around indels:

# step one: finding places to realign:

#First, index all the bam files with samtools index:

~/scripts/indexBams.sh

#Prep the input file for RealignerTargetCreator:

ls *.bam > bams
cat bams | perl -pe 's/(\S+)\.bam/java -Xmx5g -jar ~\/scripts\/GenomeAnalysisTK_3_6\.jar -T RealignerTargetCreator -R \/nobackup\/data7\/pierre\/IB14\/Arcs_GATK\/Idotea_0002_reduced_pruned_arcs_scaffold_ccn\.fasta -I $1\.bam -o $1\.intervals/' >intervals.sge

#Then open the file and make sure the paths are correct, and add a header:
#$ -cwd
#$ -q high_mem
#$ -S /bin/bash
#$ -pe mpich 1

qsub intervals.sge

# did it run for all files? is the number of *.intervals files equal the number of *.bam files?
# if not, rerun the chunk above
ll *.intervals | wc -l

# step two: realigning
cat bams | perl -pe 's/(\S+)\.bam/java -Xmx5g -jar ~\/scripts\/GenomeAnalysisTK_3_6\.jar -T IndelRealigner -R \/nobackup\/data7\/pierre\/IB14\/Arcs_GATK\/Idotea_0002_reduced_pruned_arcs_scaffold_ccn\.fasta -targetIntervals $1\.intervals -I $1\.bam -o $1\.real.bam -LOD 0\.4/' >realign.sge

#Then open the file and make sure the paths are correct, and add a header:
#$ -cwd
#$ -q high_mem
#$ -S /bin/bash
#$ -pe mpich 1

qsub realign.sge

# did it run for all files? is the number of *.intervals files equal the number of *.bam files?
# if not, rerun the chunk above
ll *.real.bam | wc -l

#----------------------------------
# launching GATK UnifiedGenotyper for round 1 (about 30 min)
# note: it is a preliminary run needed for base quality recalibration,

#Steo one - merge all the bam files into one file called merged.bam, while keeping the sample information as "read groups" 
# First - create an rg.txt file, consisting of one line per file 
# @RG	ID:SAMPLE.fq.trim	SM:SAMPLE.fq.trim	PL:Illumina
# then merge all the bam files:

samtools merge -h rg.txt merged.bam *.real.bam

## and index the merged file:

samtools index merged.bam

# Then, create the round 1 submission script with:

echo '#!/bin/bash
#$ -cwd
#$ -q high_mem
#$ -S /bin/bash
#$ -pe mpich 12
java -jar ~/scripts/GenomeAnalysisTK_3_6.jar -T UnifiedGenotyper \
-R /nobackup/data7/pierre/IB14/Arcs_GATK/Idotea_0002_reduced_pruned_arcs_scaffold_ccn.fasta -nt 12 -nct 1 \
-I merged.bam \
-o round1.vcf' >unig.sge

# And submit it with:

qsub unig.sge

#----------------------------------
# base quality score recalibration (BQSR)

# creating high-confidence (>75 quality percentile) snp sets for 
# base quality recalibration using Kyle Hernandez's tool. (ignore the warnings)
~/scripts/GetHighQualVcfs.py  -i round1.vcf --percentile 75 -o .

# recalibrating quality scores
# step one: creating recalibration reports

ls *real.bam > bams
cat bams | perl -pe 's/(\S+)\.real\.bam/java -Xmx20g -jar ~\/scripts\/GenomeAnalysisTK_3_6\.jar -T BaseRecalibrator -R \/nobackup\/data7\/pierre\/IB14\/Arcs_GATK\/Idotea_0002_reduced_pruned_arcs_scaffold_ccn\.fasta -knownSites $1\.fq\.trim_HQ\.vcf -I $1\.real\.bam -o $1\.real\.recalibration_report.grp/' >bqsr.sge

#Then open the file and make sure the paths are correct, and add a header:
#$ -cwd
#$ -q high_mem
#$ -S /bin/bash
#$ -pe mpich 1

# And submit the sge script with:
qsub bqsr.sge

# did it run for all files? is the number of *.grp files equal the number of *.real.bam files?
# if not, rerun the chunk above
ll *.real.bam | wc -l
ll *.grp | wc -l

# step two: rewriting bams according to recalibration reports

cat bams | perl -pe 's/(\S+)\.bam/java -Xmx10g -jar ~\/scripts\/GenomeAnalysisTK_3_6\.jar -T PrintReads -R \/nobackup\/data7\/pierre\/IB14\/Arcs_GATK\/Idotea_0002_reduced_pruned_arcs_scaffold_ccn\.fasta -I $1\.bam -BQSR $1\.recalibration_report.grp -o $1\.recal\.bam /' >bqsr2.sge

#Then open the file and make sure the paths are correct, and add a header:
#$ -cwd
#$ -q high_mem
#$ -S /bin/bash
#$ -pe mpich 1

# And submit the sge script with:

qsub bqsr2.sge

# did it run for all files? is the number of *.recal.bam files equal the number of *.real.bam files?
# if not, rerun the chunk above
ll *.real.bam | wc -l
ll *.recal.bam | wc -l

ls *.recal.bam > bams

#----------------------------------
# Second iteration of UnifiedGenotyper (on quality-recalibrated files)
# this time FOR REAL! 
# Misha's comment below::
# if you need indels, run the same process separately with --genotype_likelihoods_model INDEL
# I do not recommend indel tracing for 2bRAD since the tags are too short for confident indels. 
# If you still want to try, note that the subsequent recalibration stages would 
# have do be done separately for indels, 

samtools merge -h rg.txt merged_recalibrated.bam *.recal.bam

## and index the merged file:

samtools index merged_recalibrated.bam

# Then, create the submission script with:

echo '#!/bin/bash
#$ -cwd
#$ -q high_mem
#$ -S /bin/bash
#$ -pe mpich 12
java -jar ~/scripts/GenomeAnalysisTK_3_6.jar -T UnifiedGenotyper \
-R /nobackup/data7/pierre/IB14/Arcs_GATK/Idotea_0002_reduced_pruned_arcs_scaffold_ccn.fasta -nt 12 -nct 1 \
--genotype_likelihoods_model SNP \
-I merged_recalibrated.bam \
-o round2.vcf' >unig2.sge

qsub unig2.sge

# renaming samples in the vcf file, to get rid of trim-shmim etc
cat round2.vcf | perl -pe 's/\.fq\.trim//g' | perl -pe 's/^chrom/chr/' >round2.names.vcf

#----------------------------------
# variant quality score recalibration (VQSR)

# extracting SNPs that are consistently genotyped in replicates 
# and have not too many heterozygotes:
~/scripts/replicatesMatch.pl vcf=round2.names.vcf replicates=clonepairs.tab >vqsr.vcf

# determining transition-transversion ratio for true snps (will need it for tranche calibration)
vcftools --vcf vqsr.vcf --TsTv-summary
# Ts/Tv ratio: 1.379  # put your actual number into the next code chunk, --target_titv

# Recalibrating genotype calls: VQSR
# step one - creating recalibration models (30 sec)

java -jar ~/scripts/GenomeAnalysisTK_3_6.jar -T VariantRecalibrator \
-R /nobackup/data7/pierre/IB14/Arcs_GATK/Idotea_0002_reduced_pruned_arcs_scaffold_ccn.fasta -input round2.names.vcf -nt 12 \
-resource:repmatch,known=true,training=true,truth=true,prior=10  vqsr.vcf \
-an QD -an MQ -an FS -mode SNP --maxGaussians 4 \
--target_titv 1.379 -tranche 85.0 -tranche 90.0 -tranche 95.0 -tranche 99.0 -tranche 100 \
-recalFile round2.recal -tranchesFile recalibrate_SNP.tranches -rscriptFile recalibrate_SNP_plots.R 

# fixing the R script (assumes outdated version of ggplot2):
cat recalibrate_SNP_plots.R | perl -pe 's/opts\(/theme\(/'g | perl -pe 's/theme_/element_/g' | perl -pe 's/\+ theme\(title=\"model PDF\"\)//g'  >recalibrateSNPs.R
# now copy all recalibrate* files to your laptop, run the R script, examine the resulting plot and tranches.pdf

# applying recalibration:
java -jar ~/scripts/GenomeAnalysisTK_3_6.jar -T ApplyRecalibration \
-R /nobackup/data7/pierre/IB14/Arcs_GATK/Idotea_0002_reduced_pruned_arcs_scaffold_ccn.fasta -input round2.names.vcf -nt 12 \
--ts_filter_level 95.0 -mode SNP \
-recalFile round2.recal -tranchesFile recalibrate_SNP.tranches -o gatk_after_vqsr.vcf

#----------------------------------
# ALTERNATIVE to GATK-based recalibration (if tranches are weird and error models fail to converge):
# non-parametric quantile-based recalibration a-la de novo pipeline

~/scripts/replicatesMatch.pl vcf=round2.names.vcf replicates=clonepairs.tab polyonly=1 >vqsr.vcf
~/scripts/recalibrateSNPs_gatk.pl vcf=round2.names.vcf true=vqsr.vcf >gatk_after_vqsr.vcf

# your best quality filter setting is the one with maximum "gain"

#----------------------------------

# restoring original contig names and coordinates:
~/scripts/retabvcf.pl vcf=gatk_after_vqsr.vcf tab=/nobackup/data7/pierre/IB14/Arcs_GATK/Idotea_0002_reduced_pruned_arcs_scaffold_cc.tab > retab.vcf

# discarding loci with too many heterozygotes, which are likely lumped paralogs
# (by default, fraction of heterozygotes should not exceed maxhet=0.75)
# this step can also filter for the fraction of missing genotypes (default maxmiss=0.5)
~/scripts/hetfilter.pl vcf=retab.vcf >hetfilt.vcf

# thinning SNP dataset - leaving one snp per tag
# by default, it will leave the SNP with the highest minor allele frequency; this
# is good for ADMIXTURE or Fst analysis
# if you plan to use dadi, however, use it with the option criterion=maxDP-random
~/scripts/thinner.pl vcf=hetfilt.vcf > thin.vcf

# applying filter and selecting polymorphic biallelic loci genotyped in 80% or more individuals
# for parametric (GATK-based) recalibration"
vcftools --vcf thin.vcf --remove-filtered-all --max-missing 0.8  --min-alleles 2 --max-alleles 2 --recode --recode-INFO-all --out filt0
#After filtering, kept 40460 out of a possible 73062 Sites

# for non parametric (GATK-based) recalibration: replace --minQ 15 in the following line
# with the quantile of the highest "gain" as reported by recalibrateSNPs_gatk.pl
#vcftools --vcf thin.vcf --minQ 15 --max-missing 0.8  --min-alleles 2 --max-alleles 2 --recode --recode-INFO-all --out filt0

# genotypic match between pairs of replicates 
# (the most important one is the last one, Heterozygote Discovery Rate)	
~/scripts/repMatchStats.pl vcf=filt0.recode.vcf replicates=clonepairs.tab 
# Saved the output as repmatchstats.txt!

# creating final filtered file without clones (must list them in the file clones2remove):
# (for non-parametric recalibration, replace --remove-filtered-all with --minQ [quantile of the highest gain] )
vcftools --vcf thin.vcf --remove clones2remove --remove-filtered-all --max-missing 0.8  --min-alleles 2 --max-alleles 2 --recode --recode-INFO-all --out final

# After filtering, kept 622 out of 744 Individuals
# After filtering, kept 34709 out of a possible 55963 Sites

#Finally, output all kinds of statistics with vcftools:
vcftools --vcf final.recode.vcf --out IB14_Arcs_GATK_v1_SNPs --hardy
vcftools --vcf final.recode.vcf --out IB14_Arcs_GATK_v1_SNPs --012
vcftools --vcf final.recode.vcf --out IB14_Arcs_GATK_v1_SNPs --depth
vcftools --vcf final.recode.vcf --out IB14_Arcs_GATK_v1_SNPs --site-mean-depth
vcftools --vcf final.recode.vcf --out IB14_Arcs_GATK_v1_SNPs --het
vcftools --vcf final.recode.vcf --out IB14_Arcs_GATK_v1_SNPs --missing-indv
