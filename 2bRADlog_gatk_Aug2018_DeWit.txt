### Full-blown GATK pipeline for 2bRAD data, based on bowtie2 mapping to a reference genome.
### Based on the 2014 pipeline published by Mikhail Matz, which can be found at: https://github.com/z0on/2bRAD_GATK
### September 6, 2018, Pierre De Wit, pierre.de_wit@marine.gu.se

# setting up the genome reference

# Using the latest Idotea assembly, with contigs renamed by Tomas Larsson (August 2018): 

# "Idotea_PBJelly_ArcsRound2_sealed_scaffold_short_names.fa.gz" 

# Renaming the assembly file:

gunzip Idotea_PBJelly_ArcsRound2_sealed_scaffold_short_names.fa.gz 
mv Idotea_PBJelly_ArcsRound2_sealed_scaffold_short_names.fa Ibalt_genome_Aug2018.fasta

## Concatenating genome contigs into small number of "pseudo-chromosomes" (to help memory usage in GATK).

~/scripts/concatFasta.pl fasta=Ibalt_genome_Aug2018.fasta

# normalize fasta records to ensure all lines are of the same length, using Picard

module load picard/v2.1.1

java -Xmx1g -jar ~/scripts/NormalizeFasta.jar INPUT=Ibalt_genome_Aug2018_cc.fasta OUTPUT=Ibalt_genome_Aug2018_ccn.fasta

# creating genome indexes:

module load Bowtie2/v2.2.7
bowtie2-build Ibalt_genome_Aug2018_ccn.fasta Ibalt_genome_Aug2018_ccn.fasta

module load samtools
samtools faidx Ibalt_genome_Aug2018_ccn.fasta

java -jar ~/scripts/CreateSequenceDictionary.jar R=Ibalt_genome_Aug2018_ccn.fasta  O=Ibalt_genome_Aug2018_ccn.dict

#----------------------------------
# Copying and unzipping the pre-trimmed read files:

./filecopy.sh

gunzip *.gz

#----------------------------------
# mapping reads and reformatting mapped data files

# aligning with bowtie2 :

~/scripts/2bRAD_bowtie2_launch.pl '\.trim$' /proj/data19/pierre/IB14/2018/Ibalt_genome_Aug2018_ccn.fasta > bt2_aln.sge

# You can add a multithreading flag (-p 16) to the sge script with:

sed -i 's/-L 16/-L 16 -p 16/g' bt2_aln.sge

#Then open the file and make sure the path is correct, and add a header:
#$ -cwd
#$ -q node0
#$ -S /bin/bash
#$ -pe mpich 16
module load Bowtie2/v2.2.7

# Re-save the file, then submit the job to the queue:

qsub bt2.sge

ls *.bt2.sam > sams
cat sams | wc -l  
# do you have sams for all your samples?... If not, rerun the chunk above

# making bam files

~/scripts/AddReadGroups.sh
~/scripts/convert_to_bam.sh

#rm *sorted*
#ls *bt2.bam > bams
#cat bams | wc -l  
# do you have bams for all your samples?... If not, rerun the chunk above


#----------------------------------
# starting GATK
# realigning around indels:

# step one: finding places to realign:

#First, index all the bam files with samtools index:

~/scripts/indexBams.sh

#Prep the input file for RealignerTargetCreator:

ls *.bam > bams
cat bams | perl -pe 's/(\S+)\.bam/java -Xmx5g -jar ~\/scripts\/GenomeAnalysisTK_3_6\.jar -T RealignerTargetCreator -R \/proj\/data19\/pierre\/IB14\/2018\/Ibalt_genome_Aug2018_ccn\.fasta -I $1\.bam -o $1\.intervals/' >intervals.sge

#Then open the file and make sure the paths are correct, and add a header:
#$ -cwd
#$ -q node0
#$ -S /bin/bash
#$ -pe mpich 1

qsub intervals.sge

# did it run for all files? is the number of *.intervals files equal the number of *.bam files?
# if not, rerun the chunk above
ll *.intervals | wc -l

# step two: realigning
cat bams | perl -pe 's/(\S+)\.bam/java -Xmx5g -jar ~\/scripts\/GenomeAnalysisTK_3_6\.jar -T IndelRealigner -R \/proj\/data19\/pierre\/IB14\/2018\/Ibalt_genome_Aug2018_ccn\.fasta -targetIntervals $1\.intervals -I $1\.bam -o $1\.real.bam -LOD 0\.4/' >realign.sge

#Then open the file and make sure the paths are correct, and add a header:
#$ -cwd
#$ -q node0
#$ -S /bin/bash
#$ -pe mpich 1

qsub realign.sge

# did it run for all files? is the number of *.intervals files equal the number of *.bam files?
# if not, rerun the chunk above
ll *.real.bam | wc -l

#----------------------------------
# launching GATK UnifiedGenotyper for round 1 (about 30 min)
# note: it is a preliminary run needed for base quality recalibration,

#Steo one - merge all the bam files into one file called merged.bam, while keeping the sample information as "read groups" 
# First - create an rg.txt file, consisting of one line per file 
# @RG	ID:SAMPLE.fq.trim	SM:SAMPLE.fq.trim	PL:Illumina
# then merge all the bam files:

samtools merge -h rg.txt merged.bam *.real.bam

## and index the merged file:

samtools index merged.bam

# Then, create the round 1 submission script with:

echo '#!/bin/bash
#$ -cwd
#$ -q high_mem
#$ -S /bin/bash
#$ -pe mpich 12
java -jar ~/scripts/GenomeAnalysisTK_3_6.jar -T UnifiedGenotyper \
-R /proj/data19/pierre/IB14/2018/Ibalt_genome_Aug2018_ccn.fasta -nt 12 -nct 1 \
-I merged.bam \
-o round1.vcf' >unig.sge

# And submit it with:

qsub unig.sge

#----------------------------------
# base quality score recalibration (BQSR)

# creating high-confidence (>75 quality percentile) snp sets for 
# base quality recalibration using Kyle Hernandez's tool. (ignore the warnings)
~/scripts/GetHighQualVcfs.py  -i round1.vcf --percentile 75 -o .

# recalibrating quality scores
# step one: creating recalibration reports

ls *real.bam > bams
cat bams | perl -pe 's/(\S+)\.real\.bam/java -Xmx20g -jar ~\/scripts\/GenomeAnalysisTK_3_6\.jar -T BaseRecalibrator -R \/proj\/data19\/pierre\/IB14\/2018\/Ibalt_genome_Aug2018_ccn\.fasta -knownSites $1_HQ\.vcf -I $1\.real\.bam -o $1\.real\.recalibration_report.grp/' >bqsr.sge

#Then open the file and make sure the paths are correct, and add a header:
#$ -cwd
#$ -q node0
#$ -S /bin/bash
#$ -pe mpich 1

# And submit the sge script with:
qsub bqsr.sge

# did it run for all files? is the number of *.grp files equal the number of *.real.bam files?
# if not, rerun the chunk above
ll *.real.bam | wc -l
ll *.grp | wc -l

# step two: rewriting bams according to recalibration reports

cat bams | perl -pe 's/(\S+)\.bam/java -Xmx10g -jar ~\/scripts\/GenomeAnalysisTK_3_6\.jar -T PrintReads -R \/proj\/data19\/pierre\/IB14\/2018\/Ibalt_genome_Aug2018_ccn\.fasta -I $1\.bam -BQSR $1\.recalibration_report.grp -o $1\.recal\.bam /' >bqsr2.sge

#Then open the file and make sure the paths are correct, and add a header:
#$ -cwd
#$ -q node0
#$ -S /bin/bash
#$ -pe mpich 1

# And submit the sge script with:

qsub bqsr2.sge

# did it run for all files? is the number of *.recal.bam files equal the number of *.real.bam files?
# if not, rerun the chunk above
ll *.real.bam | wc -l
ll *.recal.bam | wc -l

ls *.recal.bam > bams

#----------------------------------
# Second iteration of UnifiedGenotyper (on quality-recalibrated files)
# this time FOR REAL! 

samtools merge -h rg.txt merged_recalibrated.bam *.recal.bam

## and index the merged file:

samtools index merged_recalibrated.bam

# Then, create the submission script with:

echo '#!/bin/bash
#$ -cwd
#$ -q node0
#$ -S /bin/bash
#$ -pe mpich 12
java -jar ~/scripts/GenomeAnalysisTK_3_6.jar -T UnifiedGenotyper \
-R /proj/data19/pierre/IB14/2018/Ibalt_genome_Aug2018_ccn.fasta -nt 12 -nct 1 \
--genotype_likelihoods_model SNP \
-I merged_recalibrated.bam \
-o round2.vcf' >unig2.sge

qsub unig2.sge

# renaming samples in the vcf file, to get rid of trim-shmim etc
cat round2.vcf | perl -pe 's/\.fq\.trim//g' | perl -pe 's/^chrom/chr/' >round2.names.vcf

#----------------------------------
# variant quality score recalibration (VQSR)

# extracting SNPs that are consistently genotyped in replicates 
# and have not too many heterozygotes:
~/scripts/replicatesMatch.pl vcf=round2.names.vcf replicates=clonepairs.tab >vqsr.vcf

# determining transition-transversion ratio for true snps (will need it for tranche calibration)
vcftools --vcf vqsr.vcf --TsTv-summary
# Ts/Tv ratio: 1.348  # put your actual number into the next code chunk, --target_titv

# Recalibrating genotype calls: VQSR
# step one - creating recalibration models (30 sec)

java -jar ~/scripts/GenomeAnalysisTK_3_6.jar -T VariantRecalibrator \
-R /proj/data19/pierre/IB14/2018/Ibalt_genome_Aug2018_ccn.fasta -input round2.names.vcf -nt 8 \
-resource:repmatch,known=true,training=true,truth=true,prior=10  vqsr.vcf \
-an QD -an MQ -an FS -mode SNP --maxGaussians 4 \
--target_titv 1.348 -tranche 85.0 -tranche 90.0 -tranche 95.0 -tranche 99.0 -tranche 100 \
-recalFile round2.recal -tranchesFile recalibrate_SNP.tranches -rscriptFile recalibrate_SNP_plots.R 

# fixing the R script (assumes outdated version of ggplot2):
cat recalibrate_SNP_plots.R | perl -pe 's/opts\(/theme\(/'g | perl -pe 's/theme_/element_/g' | perl -pe 's/\+ theme\(title=\"model PDF\"\)//g'  >recalibrateSNPs.R
# now copy all recalibrate* files to your laptop, run the R script, examine the resulting plot and tranches.pdf

# applying recalibration:
java -jar ~/scripts/GenomeAnalysisTK_3_6.jar -T ApplyRecalibration \
-R /proj/data19/pierre/IB14/2018/Ibalt_genome_Aug2018_ccn.fasta -input round2.names.vcf -nt 8 \
--ts_filter_level 95.0 -mode SNP \
-recalFile round2.recal -tranchesFile recalibrate_SNP.tranches -o gatk_after_vqsr.vcf

# restoring original contig names and coordinates:
~/scripts/retabvcf.pl vcf=gatk_after_vqsr.vcf tab=/proj/data19/pierre/IB14/2018/Ibalt_genome_Aug2018_cc.tab > retab.vcf

# --------------------------

# Additional filtering steps:

# discarding loci with too many heterozygotes, which are likely lumped paralogs
# (by default, fraction of heterozygotes should not exceed maxhet=0.75)
# this step can also filter for the fraction of missing genotypes (default maxmiss=0.5)
~/scripts/hetfilter.pl vcf=retab.vcf >hetfilt.vcf

# thinning SNP dataset - leaving one snp per tag
# by default, it will leave the SNP with the highest minor allele frequency; this
# is good for ADMIXTURE or Fst analysis

~/scripts/thinner.pl vcf=hetfilt.vcf > thin.vcf

# applying filter and selecting polymorphic biallelic loci genotyped in 80% or more individuals
# for parametric (GATK-based) recalibration"
vcftools --vcf thin.vcf --remove-filtered-all --max-missing 0.8  --min-alleles 2 --max-alleles 2 --recode --recode-INFO-all --out filt0

# genotypic match between pairs of replicates 
# (the most important one is the last one, Heterozygote Discovery Rate)	
~/scripts/repMatchStats.pl vcf=filt0.recode.vcf replicates=clonepairs.tab 
# Saved the output as repmatchstats.txt!

# creating final filtered file without clones (must list them in the file clones2remove):
# (for non-parametric recalibration, replace --remove-filtered-all with --minQ [quantile of the highest gain] )
vcftools --vcf thin.vcf --remove clones2remove --remove-filtered-all --max-missing 0.8  --min-alleles 2 --max-alleles 2 --recode --recode-INFO-all --out final

# After filtering, kept 622 out of 744 Individuals
# After filtering, kept 33774 out of a possible 57641 Sites

#Finally, output all kinds of statistics with vcftools:
vcftools --vcf final.recode.vcf --out IB14_Arcs_GATK_v1_SNPs --hardy
vcftools --vcf final.recode.vcf --out IB14_Arcs_GATK_v1_SNPs --012
vcftools --vcf final.recode.vcf --out IB14_Arcs_GATK_v1_SNPs --depth
vcftools --vcf final.recode.vcf --out IB14_Arcs_GATK_v1_SNPs --site-mean-depth
vcftools --vcf final.recode.vcf --out IB14_Arcs_GATK_v1_SNPs --het
vcftools --vcf final.recode.vcf --out IB14_Arcs_GATK_v1_SNPs --missing-indv


#Then, using the idepth file, make a file with the individuals with the most missing data, called poor_data_inds.txt, and create a vcf with no missing data for PCA:

vcftools --vcf final.recode.vcf --remove poor_data_inds.txt --remove-filtered-all --max-missing 1 --recode --recode-INFO-all --out final_nomissingdata

